{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + BLIP Combined Pipeline: Multi-Modal Knowledge Graph Completion\n",
    "\n",
    "## Overview\n",
    "This notebook combines predictions from two modalities to enrich the WJoconde knowledge graph:\n",
    "- **VLM (Vision-Language Model)**: Extracts \"depicts\" relations from artwork **images**\n",
    "- **LLM (Large Language Model)**: Extracts \"depicts\" relations from artwork **text descriptions**\n",
    "\n",
    "## Workflow\n",
    "1. **Cell 1**: Combine multiple BLIP JSON output files into a single CSV\n",
    "2. **Cell 2**: Merge BLIP (vision) and LLM (text) predictions\n",
    "3. **Cell 3**: Add ground truth labels from knowledge graph for comparison\n",
    "4. **Cell 4**: Filter predictions using Word2Vec semantic similarity (threshold=0.5)\n",
    "   - Removes predictions that are semantically similar to existing KG entries\n",
    "   - Keeps only NOVEL predictions that could enrich the knowledge graph\n",
    "5. **Cell 5**: Count items in each depicts column for analysis\n",
    "6. **Cell 6**: Compute summary statistics (total predictions vs. ground truth)\n",
    "7. **Cell 7**: Count unique entities in the original knowledge graph\n",
    "\n",
    "## Key Output\n",
    "-  Final dataset containing:\n",
    "  - `from`: Entity URI\n",
    "  - `depicts`: All predictions (LLM + BLIP combined)\n",
    "  - `true_depicts`: Ground truth from knowledge graph\n",
    "  - `filtered_depicts`: Novel predictions not in KG (potential additions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Combine Multiple BLIP JSON Output Files\n",
    "# =============================================================================\n",
    "# Purpose: \n",
    "#   - Load multiple BLIP VLM output JSON files (blip_valid_json_pairs.json, etc.)\n",
    "#   - Extract \"depicts\" predictions from each entity\n",
    "#   - Merge all predictions into a single DataFrame with unique values\n",
    "#   - Save combined BLIP outputs to 'blip_outputs_all.csv'\n",
    "\n",
    "# Input: blip_valid_json_pairs.json, blip_valid_json_pairs2.json\n",
    "# Output: blip_outputs_all.csv (columns: from, depicts)\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "json_files = [\n",
    "    \"blip_valid_json_pairs.json\",\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store data\n",
    "data = {}\n",
    "\n",
    "# Process each JSON file\n",
    "for file in json_files:\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        for key, value in json_data.items():\n",
    "            if isinstance(value, dict) and \"depicts\" in value:\n",
    "                if key not in data:\n",
    "                    data[key] = set()  # Use a set to store unique depicts values\n",
    "                depicts_list = value.get(\"depicts\", [])\n",
    "                print(key)\n",
    "                print(depicts_list)\n",
    "                if isinstance(depicts_list, list):\n",
    "                    data[key].update(depicts_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Convert the cleaned data into a DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\"from\": list(data.keys()), \"depicts\": [\", \".join(sorted(depicts_set)) for depicts_set in data.values()]}\n",
    ")\n",
    "\n",
    "# Define output file path\n",
    "output_csv = \"blip_outputs_all_qwen.csv\"\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"CSV file saved at: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Merge BLIP (Vision) and LLM (Text) Predictions\n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Load BLIP predictions (from images) and LLM predictions (from text)\n",
    "#   - Merge both sources of \"depicts\" predictions for each entity\n",
    "#   - Deduplicate predictions using sets\n",
    "#   - Save the combined multi-modal predictions\n",
    "\n",
    "# Input: blip_outputs_all.csv, llm_outputs_all.csv\n",
    "# Output: llm_bilp_combined_all.csv (merged predictions from both modalities)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "blip_csv_path = \"blip_outputs_all.csv\"\n",
    "llm_csv_path = \"llm_outputs_all.csv\"\n",
    "output_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "\n",
    "# Load CSV files\n",
    "blip_df = pd.read_csv(blip_csv_path)\n",
    "llm_df = pd.read_csv(llm_csv_path)\n",
    "\n",
    "# Function to process 'depicts' column (convert to set for deduplication)\n",
    "def process_depicts(depicts):\n",
    "    if pd.isna(depicts):\n",
    "        return set()\n",
    "    return set(depicts.split(\", \"))\n",
    "\n",
    "# Create a dictionary to store combined data\n",
    "combined_data = {}\n",
    "\n",
    "# Process BLIP data\n",
    "for _, row in blip_df.iterrows():\n",
    "    key = row[\"from\"]\n",
    "    depicts_set = process_depicts(row[\"depicts\"])\n",
    "    if key not in combined_data:\n",
    "        combined_data[key] = depicts_set\n",
    "    else:\n",
    "        combined_data[key].update(depicts_set)\n",
    "\n",
    "# Process LLM data\n",
    "for _, row in llm_df.iterrows():\n",
    "    key = row[\"from\"]\n",
    "    depicts_set = process_depicts(row[\"depicts\"])\n",
    "    if key not in combined_data:\n",
    "        combined_data[key] = depicts_set\n",
    "    else:\n",
    "        combined_data[key].update(depicts_set)\n",
    "\n",
    "# Convert the merged data into a DataFrame\n",
    "final_df = pd.DataFrame(\n",
    "    {\"from\": list(combined_data.keys()), \"depicts\": [\", \".join(sorted(depicts_set)) for depicts_set in combined_data.values()]}\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"CSV file saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Add Ground Truth Labels to Combined Predictions\n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Load LLM image results containing ground truth \"depicts\" labels\n",
    "#   - Extract true_depicts from the combined_dict column\n",
    "#   - Merge ground truth labels with combined predictions\n",
    "#   - This enables comparison between predictions and ground truth\n",
    "\n",
    "# Input: llm_image_results.csv, llm_bilp_combined_all.csv\n",
    "# Output: llm_bilp_combined_all.csv (with added true_depicts column)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import ast  # To safely evaluate dictionary strings\n",
    "\n",
    "# Define file paths\n",
    "llm_image_results_path = \"llm_image_results.csv\" # just a file containing the true_depicts\n",
    "combined_outputs_path = \"blip_outputs_all.csv\"\n",
    "output_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "\n",
    "\n",
    "# Load CSV files\n",
    "llm_image_df = pd.read_csv(llm_image_results_path)\n",
    "combined_outputs_df = pd.read_csv(combined_outputs_path)\n",
    "\n",
    "# Function to safely extract keys from combined_dict\n",
    "def extract_keys(combined_dict_str):\n",
    "    try:\n",
    "        combined_dict = ast.literal_eval(combined_dict_str)  # Convert string to dictionary safely\n",
    "        if isinstance(combined_dict, dict):\n",
    "            return \", \".join(sorted(combined_dict.keys()))  # Extract and sort keys\n",
    "    except:\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "# Apply function to extract 'true_depicts'\n",
    "llm_image_df[\"true_depicts\"] = llm_image_df[\"combined_dict\"].apply(extract_keys)\n",
    "\n",
    "# Merge dataframes on 'entity' (llm_image_results) and 'from' (combined_outputs)\n",
    "final_df = combined_outputs_df.merge(\n",
    "    llm_image_df[[\"entity\", \"true_depicts\"]],\n",
    "    left_on=\"from\",\n",
    "    right_on=\"entity\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Drop redundant 'entity' column after merge\n",
    "final_df.drop(columns=[\"entity\"], inplace=True)\n",
    "\n",
    "# Save the final dataset to CSV\n",
    "final_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"CSV file saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Filter Predictions Using Word2Vec Semantic Similarity\n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Use Word2Vec (Google News 300) to compute semantic similarity\n",
    "#   - Filter out predicted \"depicts\" that are too similar to ground truth\n",
    "#   - This identifies NEW/NOVEL predictions not already in the knowledge graph\n",
    "#   - Similarity threshold: 0.4 (predictions above this are filtered out)\n",
    "\n",
    "# Method:\n",
    "#   - For each predicted depict, compare against all true_depicts\n",
    "#   - If similarity >= 0.4 with any true_depict, remove it (redundant)\n",
    "#   - Keep only predictions that are semantically distinct from ground truth\n",
    "\n",
    "# Input: llm_bilp_combined_all.csv\n",
    "# Output: llm_bilp_combined_all.csv (with filtered_depicts column)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Load the Word2Vec model (Update with the correct model path)\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Define file paths\n",
    "input_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "output_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Define similarity threshold (adjust as needed)\n",
    "similarity_threshold = 0.4\n",
    "\n",
    "# Function to process string lists\n",
    "def process_list(column_value):\n",
    "    if pd.isna(column_value) or column_value.strip() == \"\":\n",
    "        return []\n",
    "    return [x.strip() for x in column_value.split(\",\")]\n",
    "\n",
    "# Function to compute similarity for phrases\n",
    "def compute_similarity(seq, obj):\n",
    "    seq_words = [word for word in seq.split() if word in wv.key_to_index]\n",
    "    obj_words = [word for word in obj.split() if word in wv.key_to_index]\n",
    "\n",
    "    total_similarities = []\n",
    "    for seq_word in seq_words:\n",
    "        word_similarities = [\n",
    "            wv.similarity(seq_word, obj_word) \n",
    "            for obj_word in obj_words \n",
    "            if obj_word in wv.key_to_index\n",
    "        ]\n",
    "        if word_similarities:\n",
    "            total_similarities.append(max(word_similarities))  # Take max similarity for each word\n",
    "\n",
    "    return sum(total_similarities) / len(total_similarities) if total_similarities else 0\n",
    "\n",
    "# Iterate over each row with tqdm for progress tracking\n",
    "filtered_depicts_list = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Entities\", unit=\"entity\"):\n",
    "    depicts = process_list(row[\"depicts\"])\n",
    "    true_depicts = process_list(row[\"true_depicts\"])\n",
    "    \n",
    "    filtered_depicts = []\n",
    "    \n",
    "    # Iterate over each depicts phrase with tqdm\n",
    "    for depict in tqdm(depicts, desc=\"Filtering Depicts\", leave=False, unit=\"phrase\"):\n",
    "        is_similar = False\n",
    "        \n",
    "        # Compare with each true_depicts phrase\n",
    "        for true_depict in true_depicts:\n",
    "            similarity = compute_similarity(depict, true_depict)\n",
    "            if similarity >= similarity_threshold:\n",
    "                is_similar = True\n",
    "                break  # No need to check further if already above threshold\n",
    "        \n",
    "        # Keep the depict if no high similarity found\n",
    "        if not is_similar:\n",
    "            filtered_depicts.append(depict)\n",
    "    \n",
    "    # Store filtered values as a string\n",
    "    filtered_depicts_list.append(\", \".join(filtered_depicts))\n",
    "\n",
    "# Add results to the DataFrame\n",
    "df[\"filtered_depicts\"] = filtered_depicts_list\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Filtered CSV saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Count Items in Each Depicts Column\n",
    "# =============================================================================\n",
    "# Purpose:\n",
    "#   - Count the number of items in depicts, true_depicts, and filtered_depicts\n",
    "#   - Add length columns for analysis and comparison\n",
    "#   - Helps quantify: total predictions, ground truth size, novel predictions\n",
    "\n",
    "# Output columns added:\n",
    "#   - depicts_length: Number of total predicted depicts\n",
    "#   - true_depicts_length: Number of ground truth depicts\n",
    "#   - filtered_depicts_length: Number of novel (non-redundant) predictions\n",
    "# =============================================================================\n",
    "\n",
    "# Load the CSV file\n",
    "import pandas as pd\n",
    "input_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "output_csv_path = \"llm_bilp_combined_all.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Function to count the number of elements in a comma-separated list\n",
    "def count_items(column_value):\n",
    "    if pd.isna(column_value) or column_value.strip() == \"\":\n",
    "        return 0\n",
    "    return len(column_value.split(\",\"))\n",
    "\n",
    "# Compute lengths\n",
    "df[\"depicts_length\"] = df[\"depicts\"].apply(count_items)\n",
    "df[\"true_depicts_length\"] = df[\"true_depicts\"].apply(count_items)\n",
    "df[\"filtered_depicts_length\"] = df[\"filtered_depicts\"].apply(count_items)\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"CSV file saved at: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wjoconde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
